<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Speech Arena</title>
    <meta name="description" content="Distilling an End-to-End Voice Assistant from Speech Recognition Data Using Pretrained Models.">

    <!-- Facebook -->
    <meta property="og:url" content="https://salt-nlp.github.io/Speech-Arena-Project-Page/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Speech Arena">
    <meta property="og:description" content="Distilling an End-to-End Voice Assistant from Speech Recognition Data Using Pretrained Models.">
    <meta property="og:image" content="https://diva-audio.github.io/static/images/hero.png">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="diva-audio.github.io">
    <meta property="twitter:url" content="https://salt-nlp.github.io/Speech-Arena-Project-Page/">
    <meta name="twitter:title" content="DiVA (Distilled Voice Assistant)">
    <meta name="twitter:description" content="Distilling an End-to-End Voice Assistant from Speech Recognition Data Using Pretrained Models.">
    <meta name="twitter:image" content="https://diva-audio.github.io/static/images/hero.png">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" sizes="192x192" href="images/android-desktop.png">

    <meta name="viewport" content="width=device-width, height=device-height, initial-scale=1">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script
	type="module"
	src="https://gradio.s3-us-west-2.amazonaws.com/5.0.2/gradio.js"
      ></script>
  </head>
  <style>

    .SAmE {
        color: #D55E00; /* myorange */
        font-family: monospace;
        font-weight: bold;
    }
    .SAsE {
        color: #2F5596; /* mydarkblue */
        font-family: monospace;
        font-weight: bold;
    }
    .highlight {
        background-color: #F9CD69;
        font-weight: bold;
    }
    .quote {
	color: #073ea2;
	font-style: italic; 
	font-size: 1.2em;
	max-width: 50%;
	margin: 0 auto; 
	font-weight: bold; 
	margin-bottom: 20px;
	text-align: center;
    }
    table {
	max-width: 60%;
	border-collapse: collapse;
	margin: 0 auto;
	margin-top: 20px;
	text-align: center;
    }
    th, td {
	border: 1px solid #ddd;
	padding: 8px;
	font-size: 14px;
    }
    th {
	background-color: #f2f2f2;
	color: #333;
    }
    tr:nth-child(even) {
	background-color: #f9f9f9;
    }
    td:nth-child(2), td:nth-child(3) {
	text-align: center;
    }
    .dashline {
	border-top: 1px dashed #999;
    }
    caption {
	caption-side: bottom;
	font-size: 0.9em;
	padding-top: 10px;
	color: #555;
    }
    .quote-text {
	color: #073ea2;
	font-weight: bold;
    }
  </style>

  <body>
    

    <section class="hero">
      <div class="hero-body">
	<div class="container is-max-desktop">
	  <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title" style="margin-bottom:0">
		<img src="./static/speech-arena2.png" alt="A Cute Small Robot with DiVA Written on It's Chest" style="height: 10vw;"> Speech Arena</h1>
	      <h2>Rethinking the Evaluation of Large Audio Models</h2>
	      
              <div class="is-size-5 publication-authors">
		<span class="author-block">
      <a href="https://yocodeyo.github.io/">Ella Minzhi Li</a><sup><span title="Project Lead, Stanford University & National University of Singapore" alt="Project Lead, Stanford University & National University of Singapore">* †</span></sup>,</span>
		<span class="author-block"><a href="https://williamheld.com/">Will Held</a><sup><span title="Evaluation Co-Author, Georgia Institute of Technology & Stanford University" alt="Evaluation Coauthor, Georgia Institute of Technology & Stanford University">†<span></sup>,</span>
		  <span class="author-block"><a href="https://michryan.com/">Michael Ryan</a><sup><span title="Evaluation Co-Author, Stanford University" alt="Evaluation Coauthor, Stanford University">†<span></sup>,</span>
		    <span class="author-block"><a href="https://www.zhuhao.me/">Hao Zhu</a><sup><span title="Evaluation Co-Author, Stanford University" alt="Evaluation Coauthor, Stanford University">†<span></sup>,</span>
		      <span class="author-block">
			  <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a><sup><span alt="Project Advisor, Stanford University" title="Project Advisor, Stanford University" style="z-index: 1000;">** </span></sup>
			</span>
			<br style="height=0.1px"/>
			<span style="font-size: 0.8rem; position: relative; top: -1vh;">[Initial Release: Nov 11, 2024; Last Updated Nov 11, 2024]</span>
			<br style="height=0.1px"/>
              </div>
	      <div class="column has-text-centered">
		<div class="publication-links">
		  <!-- PDF Link. -->
		  <span class="link-block">
	<!--	    <a href="https://arxiv.org/abs/2410.02678"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
			<i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper Link</span>
                    </a> -->
                    
		    <!-- Code Link. -->
<!--		    <span class="link-block">
                      <a href="https://colab.research.google.com/drive/1Ab3z_BjM_FblAyne7W7hbnT6gLWOhram?usp=sharing"
			 class="external-link button is-normal is-rounded is-dark">
			<span class="icon">
			  <i class="far fa-chart-bar"></i>
			</span>
				<span>Evaluation Notebook</span>
                      </a>
		    </span>
		    <span class="link-block">  -->
                <!--      <a href="https://github.com/SALT-NLP/speech-arena"
			 class="external-link button is-normal is-rounded is-dark"> 
			<span class="icon">
			  <i class="fab fa-github"></i>
			</span>
			<span>Github</span>
                      </a>-->
		</div>
	      </div>
	      <section class="hero teaser">
		<div class="container is-max-desktop">
		  <div class="hero-body">
		    <h4 class="subtitle has-text-centered">
		      <span style="color: #820000">[TL;DR]</span> We evaluate <span
          id="highlight"><b>large audio models</b></span> 
          <br>using <span
          id="highlight"><b>18 speech comprehension datasets</b></span> and an <span
          id="highlight"><b>interactive arena platform</b></span>.</a>
        </h4>
        <br>
        <h3 class="subtitle has-text-centered">Try it out now: </h3>
        <a href="https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">  
			<i class="fas fa-network-wired"></i>
                      </span>
                      <span>Arena Link</span>
                    </a>
		  </div>
		</div>
	      </section>
            </div>
	  </div>
	</div>
    </section>

    <section class="section" style="background-color:#fafaf9">
      <div class="container">
	<!-- Abstract. -->
	<div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">Demo</h2>
	  </div>
	</div>
	<div class="columns is-centered has-text-centered">
	  <div class="container is-max-desktop">
	    <video id="teaser" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
        <source src="./website/videos/paps_demo.mp4" type="video/mp4">
    </video>
	  </div>
	</div>
      </div>
    </section>
    

    <section class="section" >
      <div class="container is-max-desktop">
	<!-- Abstract. -->
	<div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">Overview</h2>
	    
	    <img src="./static/overview.png" alt="Description of the training pipeline. The trainable red modules are the Whisper Decoder, Query Tokens, and a Projection. The frozen Blue modules are the Whisper Encoder and all of Llama." style="width: 75vw;">
            <div class="content has-text-justified">
              <p>
                <div class="columns is-centered has-text-centered"><p>Comparison between <b>Static Evaluation</b> and <b>Speech Arena</b>.</p></div>
		
		<br/><br/>

		Recently large language models have gained greater capability in other modalities such as speech. Speech is a low-fraction interface and offers more opportunities of interaction with end users.
                    There are many recent work trying to benchmark audio models with a set of carefully selected audio benchmarks. However, there lacks a platform to get real user interactions and preferences.
                    To tackle this, we introduce Speech Arena, an open platform for evaluating Large Audio Models with pairwise human preferences. It helps to reveal insights on:
                    <br><br>
                    <b>What use cases users are exploring with large audio models?</b> We can analyze user queries from the wild and compare the use case difference with traditional use cases of text LLMs.
                    <br><br>
                    <b>Which Large Language Model users prefer the most?</b> Users vote their preferences with self-initiated prompts, which better reflects the actual user experience.
                    <br><br>
                    <b>Are static speech comprehension benchmarks predictive user preferences in interactive settings?</b> It helps to reveal the gap between the mainstream evaluation method for audio models and actual user preferences.
            </div>
          </div>
	</div>        
      </div>
    </section>

    <section class="section" style="background-color:#FFFFFF">
      <div class="container is-max-desktop">
	<!-- Abstract. -->
	<div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">Static Evaluation</h2>
	    <div class="content has-text-justified">
              <p>
                We select <b>18</b> speech comprehension benchmark and perform evaluation for <b>10</b> different large audio models.
                <br><br>
                A wide range of tasks are covered, which include <b>Humor Detection</b>, <b>Sarcasm Detection</b>, <b>Intent Detection</b>, <b>Relationship Classification</b>, <b>Gender Classification</b>, <b>Age Classification</b>, <b>Accent Classification</b>, <b>Speech Grounding</b>, <b>Language Identification</b>, <b>Speech Entity Recognition</b>, <b>Speech Question Answering</b>, and <b>Speech Instruction Following</b>.
	      </p>
            </div>
	    <img src="./static/tasks.png" alt="Tasks and Datasets for Static Evaluation." style="height: 20vw;">
            <div class="content has-text-justified">
              <p>
                To ensure robustness, we report the average of model performance using three different prompt variations. For public_sg_speech, openhermes, and alpaca datasets, we report the cfm. For other tasks, we report the macro F1 scores.
		
	      </p>
            </div>
	    <img src="./static/results.png" alt="Speech Translation results on COVOST2. Qwen 2 performs the strongest on the most tasks, with DiVA coming in at second." style="height: 30vw;">
            <div class="content has-text-justified">
              <p>
                Close sourced models like <a href="https://arxiv.org/abs/2312.11805">Gemini</a> and <a href="https://arxiv.org/abs/2312.11805">GPT4o</a> generally tops the leaderboard. For open-sourced models, 

	      </p>
            </div>
	    
            <div class="content has-text-justified">
              
            </div>
	    <div class="content has-text-justified">		
              
	    </p>
            </div>
          </div>
	</div>
	

        
      </div>
    </section>

    <section class="section" style="background-color:#ffffff">
      <div class="container is-max-desktop">
	<!-- Abstract. -->
	<div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">Interactive Evaluation</h2>
	    <div class="content has-text-justified">
        <h3 class="title is-4">(A) User Preference</h3>
              <p>
		As an initial effort, we collected over 500 votes using the arena with pairwise comparisons between Qwen2-Audio and DiVA (reported in <a href="https://arxiv.org/abs/2410.02678">Diva paper</a>).
        
  </p>
            </div>
	    <img src="./static/user_study.svg" alt="QA results on Spoken Dialect QA and HeySQUAD. DiVA significantly (0.05) outperforms SALMONN and both Qwen Audio models." style="width: 40vw; height: 10vw;">
            <div class="content has-text-justified">
              <p>
		DiVA is significantly preferred by users over Qwen2-Audio with a 72% win rate.

<h3 class="title is-4">(B) Comparison with Static Evaluation</h3>
		We compare the result to that of static evaluation:
	      </p>
            </div>
	    <img src="./static/results.png" alt="Speech Translation results on COVOST2. Qwen 2 performs the strongest on the most tasks, with DiVA coming in at second." style="height: 30vw;">
            <div class="content has-text-justified">
              <p>
    We can see that some static evaluation results align with the direction of user preference, for tasks like humor detection, emotion recognition, and relationship classification.

		<br/>
Interestingly, for speech question answering tasks like cn-college-listen, Qwen2-Audio performs significantly better than DiVA but it is not reflective enought of the user preference.		
<h3 class="title is-4">(C) User Queries</h3>
We also perform qualitative analysis for user voice queries and obtained some insights:
<br>
<b>Query Analysis.</b> 
<br>
We identify three common types of user queries:
<br>
1) <b>Pragmatic queries</b> like resume building tips, weather and time checking. These queries are mostly short (one sentence only). It usually requires the audio model to ask more about the context to provide a personalized response.
<br>
2) <b>Knowledge expansion</b> questions like asking about theory of relativity. These queries need the audio model to be well equipped with a diverse and comprehensive knowledge base to provide factual and detailed information across a wide range of topics.
<br>
3) <b>Personal sharing</b> on recent dilemma, personal opinions and habits. These queries are often longer and more elaborated. To provide good responses, the voice assistant may need to make inferences about the users' tone and capture details well enough.
<br>
<b>Difference from text queries.</b> 
<br>
Unlike how users interact with text-only LLMs through typing, we found some unique aspects in speech interaction:
<br>
1) <b>Background noises</b> like resume building tips, weather and time checking. These queries are mostly short (one sentence only). It usually requires the audio model to ask more about the context to provide a personalized response.
<br>
2) <b>Vocal cues</b> questions like asking about theory of relativity. These queries need the audio model to be well equipped with a diverse and comprehensive knowledge base to provide factual and detailed information across a wide range of topics.
<br>
3) <b>Personal sharing</b> on recent dilemma, personal opinions and habits. These queries are often longer and more elaborated. To provide good responses, the voice assistant may need to make inferences about the users' tone and capture details well enough.

</p>
            </div>



          </div>
	</div>
	

        
      </div>
    </section>

<section class="section" style="background-color:#ffffff">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Ethics and Disclosure</h2>
          <div class="content has-text-justified">
            This study has been approved by the Institutional Review Board (IRB) at the researchers' institution, and we obtained participant consent with a standard institutional consent form.
  
          </div>
        </div>
      </div>
        
    </div>
  </section>
    
    <section class="section" id="BibTeX" style="background-color:#fafaf9">
      <div class="container is-max-desktop content">
	<h2 class="title">BibTeX</h2>
	<pre><code>
    @misc{li2024speecharena,
      title={Speech Arena: Rethinking the Evaluation of Large Audio Models}, 
      author={Minzhi Li and Will Held and Michael Ryan and Hao Zhu and Diyi Yang},
      year={2024}
    }
	</code></pre>
      </div>
    </section>


    <footer class="footer">
      <div class="container">
	<div class="columns is-centered">
	  <div class="column is-8">
            <div class="content">
	      <p> We appreciate <a href="https://x.com/dlwh">David Hall</a> for his advice and code review on the core Levanter Framework and implementation of Audio Infrastructure in Levanter. We additionally appreciate the <a href="https://sites.research.google/trc/about/">TPU Research Cloud</a> for their free computing support without which this projet would not be possible. Further computing support was provided by <a href="https://hai.stanford.edu/call-google-cloud-credit-proposals">Stanford HAI Google Cloud Credit Program</a>
	      
              <p>
		This website is licensed under a <a rel="license"
                                                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
		  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
              <p>
		The source code of this website is borrowed from <a
								    href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
              </p>
            </div>
	  </div>
	</div>
      </div>
    </footer>

  </body>
</html>
